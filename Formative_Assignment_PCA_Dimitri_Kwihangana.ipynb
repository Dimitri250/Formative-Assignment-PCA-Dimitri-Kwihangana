{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dimitri250/Formative-Assignment-PCA-Dimitri-Kwihangana/blob/main/Formative_Assignment_PCA_Dimitri_Kwihangana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary package\n",
        "#TO DO\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**[TO DO: Insert Answer here]**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(data,axis=0)\n",
        "std_dev = np.std(data, axis=0)\n",
        "standardized_data = (data - mean) / std_dev\n",
        "print (standardized_data)\n"
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c79b401-3004-4c64-c592-86bc7985f098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling data on the same scale is crucial because it ensures that all features contribute equally to the analysis. Without standardization, features with larger scales would dominate the results, leading to biased and inaccurate conclusions. This equalizes the influence of each feature, allowing algorithms like PCA to accurately identify the true patterns and relationships in the data, rather than being misled by the magnitude of the feature values"
      ],
      "metadata": {
        "id": "YxSLUhPyHoHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "85396939-f5c1-42e5-a0ce-801ee18d01de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "print(\"Eigenvalues:\\n\", eigenvalues)\n",
        "print(\"Eigenvectors:\\n\", eigenvectors)\n",
        "\n"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0110e1-f35c-4992-d86b-d540a3b14e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            " [4.74189469e-05 4.94531029e-02 4.04085720e-01 1.73655615e+00\n",
            " 3.80985761e+00]\n",
            "Eigenvectors:\n",
            " [[ 0.28128049  0.70733581 -0.03317471 -0.45182808 -0.4640131 ]\n",
            " [ 0.6706731  -0.29051532 -0.15803498 -0.48800851  0.45019005]\n",
            " [ 0.24186072  0.48462321 -0.5029143   0.55665017  0.37929082]\n",
            " [-0.03373724 -0.36999674 -0.78311558 -0.03162214 -0.4976889 ]\n",
            " [-0.64143906  0.20861365 -0.32822489 -0.49682965  0.43642295]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "# Sort eigenvalues and eigenvectors\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print('The order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "print('\\n\\n Sorted eigenvalues:\\n{}'.format(sorted_eigenvalues))\n",
        "\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]  # Sort the columns\n",
        "print('\\n\\n The sorted eigenvector matrix is: \\n {}'.format(sorted_eigenvectors))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "d62ed78c-2fff-4595-9f45-48c5fd7fd624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The order of importance is :\n",
            " [4 3 2 1 0]\n",
            "\n",
            "\n",
            " Sorted eigenvalues:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigenvector matrix is: \n",
            " [[-0.4640131  -0.45182808 -0.03317471  0.70733581  0.28128049]\n",
            " [ 0.45019005 -0.48800851 -0.15803498 -0.29051532  0.6706731 ]\n",
            " [ 0.37929082  0.55665017 -0.5029143   0.48462321  0.24186072]\n",
            " [-0.4976889  -0.03162214 -0.78311558 -0.36999674 -0.03373724]\n",
            " [ 0.43642295 -0.49682965 -0.32822489  0.20861365 -0.64143906]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "    Answer: Ordering eigenvalues and eigenvectors allows us to identify and prioritize the principal components that contribute the most variance to the dataset, aiding in effective dimensionality reduction and data interpretation\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "    Answer:No, it's not true that we would consider the lowest eigenvalue compared to the highest. In fact, we prioritize the eigenvectors corresponding to the highest eigenvalues because they capture the most variance in the data. This is because higher eigenvalues signify higher variance along those principal components, making them more important for retaining the essential information in the dataset during dimensionality reduction\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = (sorted_eigenvalues / np.sum(sorted_eigenvalues)) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print(explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "a43bbf17-9fdc-4ded-9a6c-27a716b12910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2 # select the number of principal components\n",
        "\n",
        "reduced_data = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data, reduced_data)"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "7d1e005f-bd39-4225-ee66-a35bdd9d28a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.00968857  0.09104374]\n",
            " [-0.00174858 -0.13085023]\n",
            " [ 0.0097965   0.25334771]\n",
            " [-0.00482511  0.14501926]\n",
            " [ 0.00315329 -0.36948581]\n",
            " [ 0.00331248  0.01092534]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "fbc7a1c8-9b6a-4e04-a666-7b91a89b8560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "\n",
        "**Positive Effects**\n",
        "\n",
        "**1.Dimensionality Reduction:** PCA reduces the number of features while retaining most of the dataset's variance, simplifying models and improving computational efficiency.\n",
        "\n",
        "**2.Noise Reduction**: By focusing on principal components with the highest variance, PCA can filter out noise and improve the signal-to-noise ratio in the data.\n",
        "\n",
        "**Negative Effects of PCA**\n",
        "\n",
        "**1.Loss of Interpretability:** The transformed principal components are linear combinations of the original features, making it difficult to interpret their meanings.\n",
        "\n",
        "**2.Information Loss:** If too many principal components are discarded, significant information might be lost, potentially affecting the model's performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}